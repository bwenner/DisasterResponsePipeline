{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import DataScienceHelperLibrary as dsh\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterConfig:\n",
    "    Database = 'MyDisasterResponse.db'\n",
    "    Table = 'Message'\n",
    "    SaveFileJoblib = 'MyDisasterPredictor.pkl'\n",
    "    SaveFilePickle = 'MyDisasterPredictor.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "database = 'MyDisasterResponse.db'\n",
    "table = 'Message'\n",
    "\n",
    "engine = create_engine('sqlite:///{}'.format(DisasterConfig.Database))\n",
    "\n",
    "df = pd.read_sql_table(DisasterConfig.Table, engine)\n",
    "#df = pd.read_sql(\"SELECT * FROM {}\".format(table), engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Dataframe analysis started --------------------\n",
      "Shape:  (26216, 40)\n",
      "Number of duplicate rows:  0\n",
      "-------------------- Analysis of Columns with NaN values --------------------\n",
      "Columns having all values: 39, 97.50%\n",
      "id, type: int64                        0.0\n",
      "message, type: object                  0.0\n",
      "genre, type: object                    0.0\n",
      "related, type: int64                   0.0\n",
      "request, type: int64                   0.0\n",
      "offer, type: int64                     0.0\n",
      "aid_related, type: int64               0.0\n",
      "medical_help, type: int64              0.0\n",
      "medical_products, type: int64          0.0\n",
      "search_and_rescue, type: int64         0.0\n",
      "security, type: int64                  0.0\n",
      "military, type: int64                  0.0\n",
      "child_alone, type: int64               0.0\n",
      "water, type: int64                     0.0\n",
      "food, type: int64                      0.0\n",
      "shelter, type: int64                   0.0\n",
      "clothing, type: int64                  0.0\n",
      "money, type: int64                     0.0\n",
      "missing_people, type: int64            0.0\n",
      "refugees, type: int64                  0.0\n",
      "death, type: int64                     0.0\n",
      "other_aid, type: int64                 0.0\n",
      "infrastructure_related, type: int64    0.0\n",
      "transport, type: int64                 0.0\n",
      "buildings, type: int64                 0.0\n",
      "electricity, type: int64               0.0\n",
      "tools, type: int64                     0.0\n",
      "hospitals, type: int64                 0.0\n",
      "shops, type: int64                     0.0\n",
      "aid_centers, type: int64               0.0\n",
      "other_infrastructure, type: int64      0.0\n",
      "weather_related, type: int64           0.0\n",
      "floods, type: int64                    0.0\n",
      "storm, type: int64                     0.0\n",
      "fire, type: int64                      0.0\n",
      "earthquake, type: int64                0.0\n",
      "cold, type: int64                      0.0\n",
      "other_weather, type: int64             0.0\n",
      "direct_report, type: int64             0.0\n",
      "dtype: float64\n",
      "Columns having > 50% and <= 70% missing values: 1, 2.50%\n",
      "original, type: object    0.612069\n",
      "dtype: float64\n",
      "-------------------- Analysis of Columns with NaN values finished --------------------\n",
      "Considering columns:  ['id', 'message', 'original', 'genre', 'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
      "-------------------- Dataframe value counts analye started --------------------\n",
      "********************  ********************\n",
      "More than 20 different values:  26180\n",
      "Name:  id , dtype:  int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "More than 20 different values:  26177\n",
      "Name:  message , dtype:  int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "More than 20 different values:  9630\n",
      "Name:  original , dtype:  int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "news      13054\n",
      "direct    10766\n",
      "social     2396\n",
      "Name: genre, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "1    19906\n",
      "0     6122\n",
      "2      188\n",
      "Name: related, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    21742\n",
      "1     4474\n",
      "Name: request, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    26098\n",
      "1      118\n",
      "Name: offer, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    15356\n",
      "1    10860\n",
      "Name: aid_related, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24132\n",
      "1     2084\n",
      "Name: medical_help, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24903\n",
      "1     1313\n",
      "Name: medical_products, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25492\n",
      "1      724\n",
      "Name: search_and_rescue, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25745\n",
      "1      471\n",
      "Name: security, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25356\n",
      "1      860\n",
      "Name: military, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    26216\n",
      "Name: child_alone, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24544\n",
      "1     1672\n",
      "Name: water, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    23293\n",
      "1     2923\n",
      "Name: food, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    23902\n",
      "1     2314\n",
      "Name: shelter, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25811\n",
      "1      405\n",
      "Name: clothing, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25612\n",
      "1      604\n",
      "Name: money, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25918\n",
      "1      298\n",
      "Name: missing_people, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25341\n",
      "1      875\n",
      "Name: refugees, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25022\n",
      "1     1194\n",
      "Name: death, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    22770\n",
      "1     3446\n",
      "Name: other_aid, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24511\n",
      "1     1705\n",
      "Name: infrastructure_related, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25015\n",
      "1     1201\n",
      "Name: transport, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24883\n",
      "1     1333\n",
      "Name: buildings, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25684\n",
      "1      532\n",
      "Name: electricity, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    26057\n",
      "1      159\n",
      "Name: tools, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25933\n",
      "1      283\n",
      "Name: hospitals, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    26096\n",
      "1      120\n",
      "Name: shops, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25907\n",
      "1      309\n",
      "Name: aid_centers, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25065\n",
      "1     1151\n",
      "Name: other_infrastructure, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    18919\n",
      "1     7297\n",
      "Name: weather_related, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24061\n",
      "1     2155\n",
      "Name: floods, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    23773\n",
      "1     2443\n",
      "Name: storm, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25934\n",
      "1      282\n",
      "Name: fire, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    23761\n",
      "1     2455\n",
      "Name: earthquake, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    25686\n",
      "1      530\n",
      "Name: cold, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    24840\n",
      "1     1376\n",
      "Name: other_weather, dtype: int64\n",
      "********************  ********************\n",
      "********************  ********************\n",
      "0    21141\n",
      "1     5075\n",
      "Name: direct_report, dtype: int64\n",
      "********************  ********************\n",
      "-------------------- Dataframe value counts analysis finished --------------------\n",
      "-------------------- Dataframe analysis finished --------------------\n"
     ]
    }
   ],
   "source": [
    "dsh.AnalyzeDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... there were nan values that have been inserted into database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, message, original, genre, related, request, offer, aid_related, medical_help, medical_products, search_and_rescue, security, military, child_alone, water, food, shelter, clothing, money, missing_people, refugees, death, other_aid, infrastructure_related, transport, buildings, electricity, tools, hospitals, shops, aid_centers, other_infrastructure, weather_related, floods, storm, fire, earthquake, cold, other_weather, direct_report]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.original == np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.... there were definitely missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.at[7408, 'original']) # example ID I radonmly found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, message, original, genre, related, request, offer, aid_related, medical_help, medical_products, search_and_rescue, security, military, child_alone, water, food, shelter, clothing, money, missing_people, refugees, death, other_aid, infrastructure_related, transport, buildings, electricity, tools, hospitals, shops, aid_centers, other_infrastructure, weather_related, floods, storm, fire, earthquake, cold, other_weather, direct_report]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.original == None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.... I thought that None or np.nan would be interpretet as something like DBNULL. Instead it is a string value?! I didn't calculated that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, message, original, genre, related, request, offer, aid_related, medical_help, medical_products, search_and_rescue, security, military, child_alone, water, food, shelter, clothing, money, missing_people, refugees, death, other_aid, infrastructure_related, transport, buildings, electricity, tools, hospitals, shops, aid_centers, other_infrastructure, weather_related, floods, storm, fire, earthquake, cold, other_weather, direct_report]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.original == 'NoneType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I give up... I thought there is an adequate handling for np.nan und database NULL values. I just intended to check the number of mising values before and after writing into database. But as you can see, currently I'm out of ideas how to do it ... And I don't intend to waste time with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark: Cleaning again\n",
    "Before I split data, I remove those rows that would be null/none/empty after going through Tokenize() to avoid bad side effects. For example I don't konw how a training message is treated when it has no weights and is assigned to some categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned at ETL 7.0, I didn't drop rows that were irrelevant because its possible that irrelevant rows have been added before selecting the data from database. \n",
    "\n",
    "So here at this step the data needs to be cleaned either way. My following steps:\n",
    "- remove column 'original' as there are many missing values as explored during ETL and cannot be proven here as you can see above ^^\n",
    "- remove rows when message starts with 'NOTES:'\n",
    "- remove rows with related not in [0, 1]\n",
    "- remove rows that have no category at all\n",
    "- remove rows with messages that are much longer than the length of all messages (I intend to predict messages, no prosa texts)\n",
    "- remove rows that would be empty after message has gone through Tokenize( ) so please allow me at this place to slightly change the order of to do's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start finding and removing columns matchting to wildcards: ['original'] --------------------\n",
      "Columns found to remove:  ['original']\n",
      "-------------------- Finished removing columns matchting to wildcards --------------------\n"
     ]
    }
   ],
   "source": [
    "df = dsh.RemoveColumnsByWildcard(df, 'original')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[df['message'].str.startswith('NOTES:')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Remove messages starting with 'NOTES:'\n",
    "\n",
    "df = dsh.RemoveRowsWithValueInColumn(df, 'message', ['NOTES:'], option = 'startswith')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 rows (ca. 0.72%) have been removed not having value/s \"[0, 1]\" in column \"related\"\n"
     ]
    }
   ],
   "source": [
    "# Just keep messages with related = 0 or 1 (drop 2 or potentially other values)\n",
    "\n",
    "df = dsh.SelectRowsWithValueInColumn(df, 'related', [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove rows that have no category at all\n",
    "\n",
    "# # Get all categories\n",
    "# colsToSum = list(dsh.RemoveColumnsByWildcard(df, ['index', 'id', 'message', 'original', 'genre']))\n",
    "\n",
    "# shapeBefore = df.shape\n",
    "# dfRemovedNoCategory = df[df[colsToSum].sum(axis = 1) == 0]\n",
    "# df = df[df[colsToSum].sum(axis = 1) > 0]\n",
    "\n",
    "# print('Rows removed that had no category: ', shapeBefore[0] - df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove messages that are much longer than the average\n",
    "\n",
    "#df = dsh.RemoveRowsByValuesOverAverage(df, 'message', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerSettings:\n",
    "    \n",
    "    SupportedLanguages = [\n",
    "    'english', \n",
    "    #'frensh',\n",
    "    #'haitian'\n",
    "    ]\n",
    "    \n",
    "    ConsiderOnlyLetters = 0\n",
    "    ConsiderOnlyLettersNumbers = 0\n",
    "    ConsiderMinimumLength = 0\n",
    "    \n",
    "    ReplaceUrlWithPlaceHolder = False\n",
    "    CleanUrlReg = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    UrlPlaceHolder = ''    \n",
    "    \n",
    "    RemovePunctuation = True\n",
    "    RemovePuncReg = r'[^a-zA-Z0-9]'\n",
    "    \n",
    "    UseLemmatizer = 0\n",
    "    UseStemmer = 1\n",
    "    UseStopWords = 1\n",
    "    UseTokenizer = 1\n",
    "    UseWordNet = 1\n",
    "    \n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    Stemmer = PorterStemmer()\n",
    "    \n",
    "    # If using the option UseWordNet:\n",
    "    \n",
    "    # By hazard I saw a message with that organization/institution,\n",
    "    # filtered roughtly 40 rows and decided to consider/give this word a weight\n",
    "    # and not just drop it.\n",
    "    # So I intended to make it parameterizable.\n",
    "    WhiteListWords = set(['UNHCR'])\n",
    "    \n",
    "\n",
    "    \n",
    "def tokenize(text):\n",
    "    \n",
    "    text = text.lower() # I prefer calling this function once instead for each word\n",
    "    \n",
    "    if TokenizerSettings.ReplaceUrlWithPlaceHolder:\n",
    "        if not dsh.IsNullOrEmpty(TokenizerSettings.CleanUrlReg):\n",
    "            foundUrls = re.findall(TokenizerSettings.CleanUrlReg, text)\n",
    "\n",
    "            for url in foundUrls:\n",
    "                text = text.replace(url, TokenizerSettings.UrlPlaceHolder)\n",
    "    \n",
    "    if TokenizerSettings.RemovePunctuation:\n",
    "        text = re.sub(TokenizerSettings.RemovePuncReg, ' ', text.lower())\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Call str() if values have been passed to whitelistwords that are no strings\n",
    "    whiteList = [str(white).lower() for white in TokenizerSettings.WhiteListWords]\n",
    "    \n",
    "    cleanTokens = []\n",
    "    for tok in tokens:\n",
    "\n",
    "        if TokenizerSettings.ConsiderOnlyLetters:\n",
    "            tok = dsh.KeepLetters(tok)\n",
    "        elif TokenizerSettings.ConsiderOnlyLettersNumbers:\n",
    "            tok = dsh.KeepLettersNumbers(tok)\n",
    "            \n",
    "        if TokenizerSettings.UseLemmatizer:\n",
    "            tok = TokenizerSettings.Lemmatizer.lemmatize(tok)\n",
    "        \n",
    "        if TokenizerSettings.UseStemmer:\n",
    "            stemmed = TokenizerSettings.Stemmer.stem(tok)\n",
    "        \n",
    "        if len(tok) < TokenizerSettings.ConsiderMinimumLength:\n",
    "            #print('Too short')\n",
    "            continue\n",
    "        \n",
    "        isStopWord = False\n",
    "        if TokenizerSettings.UseStopWords:\n",
    "            try:\n",
    "                for lang in list([lng for lng in TokenizerSettings.SupportedLanguages if lng is not None and len(lng) > 0]):\n",
    "                    if tok in stopwords.words(lang):\n",
    "                        isStopWord = True\n",
    "                        break\n",
    "            except:\n",
    "                print('Error during stop word handling with language: ', lang, \\\n",
    "                    '\\nand token: ', tok)\n",
    "        if isStopWord:\n",
    "            #print('Stopword')\n",
    "            continue\n",
    "        \n",
    "        isUnknownWord = False\n",
    "        if TokenizerSettings.UseWordNet:\n",
    "            if not wordnet.synsets(tok):\n",
    "                if not tok in whiteList:\n",
    "                    isUnknownWord = True\n",
    "        if isUnknownWord:\n",
    "            continue\n",
    "        \n",
    "        cleanTokens.append(tok)\n",
    "    \n",
    "    return cleanTokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now remove those rows with empty message after going through Tokenize()\n",
    "dsh.PrintLine('Removing potentially empty rows after Tokenize')\n",
    "print('Please be paitent, this step may take some seconds')\n",
    "df['dropRow'] = df['message'].apply(lambda x: 1 if len(tokenize(x)) == 0 else 0)\n",
    "dsh.PrintLine('Finished')\n",
    "\n",
    "df[df['dropRow'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these 37 \"useless\" messages can smoothly be dropped."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = dsh.RemoveRowsWithValueInColumn(df, 'dropRow', 1)\n",
    "\n",
    "df = df.drop('dropRow', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start finding and keeping columns matchting to wildcards: ['message'] --------------------\n",
      "-------------------- Finished keeping columns matchting to wildcards: 1 --------------------\n",
      "-------------------- Start finding and removing columns matchting to wildcards: ['id', 'message', 'original', 'genre'] --------------------\n",
      "Columns found to remove:  ['id', 'message', 'genre']\n",
      "-------------------- Finished removing columns matchting to wildcards --------------------\n"
     ]
    }
   ],
   "source": [
    "X = dsh.SelectColumnsByWildcard(df, ['message'])\n",
    "\n",
    "Y = dsh.RemoveColumnsByWildcard(df, ['id', 'message', 'original', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cv = CountVectorizer(tokenizer = tokenize)\n",
    "_tfidf = TfidfTransformer()\n",
    "\n",
    "_classifierInner = RandomForestClassifier(\n",
    "    #n_estimators = 10,\n",
    "    #min_samples_split = 10,\n",
    "#    random_state = 42\n",
    ")\n",
    "_classifierOuter = MultiOutputClassifier(_classifierInner)\n",
    "\n",
    "pipeline = Pipeline([('vect', _cv),\n",
    "                     ('tfidf', _tfidf),\n",
    "                     ('clf', _classifierOuter,)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Test/Train split (test size = 0.3, random state = 21: --------------------\n",
      "Training: X = (18219, 1), y = (18219, 36)\n",
      "Test    : X = (7809, 1), y = (7809, 36)\n",
      "-------------------- - --------------------\n"
     ]
    }
   ],
   "source": [
    "XTrain, XTest, yTrain, yTest = dsh.SplitDataTrainTest(X, Y, randomState = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start fitting model to data --------------------\n",
      "-------------------- Train time: 0:02:21.389836 --------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(63)\n",
    "model = dsh.TrainModel(pipeline, XTrain['message'].values, yTrain.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModel(model, xTest, yTest, columns, printIncorrect = False):\n",
    "    \n",
    "    yPred = model.predict(xTest)\n",
    "    \n",
    "    dicClassifyReports = {}\n",
    "    \n",
    "    for ind in range(yPred.shape[1]):\n",
    "        colName = columns[ind]\n",
    "        dsh.PrintLine('Column: ' + str(colName))\n",
    "        cr = classification_report(yTest[:, ind], yPred[:, ind])\n",
    "        dicClassifyReports[colName] = cr\n",
    "        print(cr)\n",
    "        \n",
    "#         cntErr = 0\n",
    "#         dirError = {}\n",
    "#         lstFalses = []\n",
    "#         for cind in range(yTest.shape[1]):\n",
    "\n",
    "#             cntErr = 0\n",
    "#             colname = columns[cind]\n",
    "\n",
    "#             for ind in range(yTest.shape[0]):\n",
    "\n",
    "#                 if yTest[ind, cind] != yPred[ind, cind]:\n",
    "#                     cntErr += 1\n",
    "#                     lstFalses.append(str(colname) + ' - ' + str(xTest[ind]))\n",
    "\n",
    "#             err = cntErr * 100 / yTest.shape[0]\n",
    "#             dirError[str(colname)] = err\n",
    "\n",
    "#         dfError = pd.DataFrame({'Errors' : list(dirError.values())}, index = columns)\n",
    "#     if printIncorrect:\n",
    "#         for val in lstFalses:\n",
    "#             print(tokenize(val))\n",
    "#         print('False predicted')\n",
    "\n",
    "#     dfError.sort_values('Errors',ascending = False)\n",
    "        \n",
    "#     dsh.PrintLine()\n",
    "    \n",
    "#     return dfError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Column: related --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.43      0.51      1897\n",
      "          1       0.83      0.92      0.87      5912\n",
      "\n",
      "avg / total       0.78      0.80      0.78      7809\n",
      "\n",
      "-------------------- Column: request --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93      6476\n",
      "          1       0.76      0.48      0.59      1333\n",
      "\n",
      "avg / total       0.88      0.89      0.87      7809\n",
      "\n",
      "-------------------- Column: offer --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      7779\n",
      "          1       0.00      0.00      0.00        30\n",
      "\n",
      "avg / total       0.99      1.00      0.99      7809\n",
      "\n",
      "-------------------- Column: aid_related --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.85      0.80      4582\n",
      "          1       0.74      0.62      0.68      3227\n",
      "\n",
      "avg / total       0.75      0.75      0.75      7809\n",
      "\n",
      "-------------------- Column: medical_help --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96      7192\n",
      "          1       0.56      0.09      0.15       617\n",
      "\n",
      "avg / total       0.90      0.92      0.90      7809\n",
      "\n",
      "-------------------- Column: medical_products --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      7410\n",
      "          1       0.65      0.09      0.16       399\n",
      "\n",
      "avg / total       0.94      0.95      0.93      7809\n",
      "\n",
      "-------------------- Column: search_and_rescue --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      7586\n",
      "          1       0.54      0.03      0.06       223\n",
      "\n",
      "avg / total       0.96      0.97      0.96      7809\n",
      "\n",
      "-------------------- Column: security --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      7680\n",
      "          1       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.97      0.98      0.97      7809\n",
      "\n",
      "-------------------- Column: military --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      7534\n",
      "          1       0.52      0.05      0.09       275\n",
      "\n",
      "avg / total       0.95      0.96      0.95      7809\n",
      "\n",
      "-------------------- Column: child_alone --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      7809\n",
      "\n",
      "avg / total       1.00      1.00      1.00      7809\n",
      "\n",
      "-------------------- Column: water --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.97      7306\n",
      "          1       0.83      0.32      0.46       503\n",
      "\n",
      "avg / total       0.95      0.95      0.94      7809\n",
      "\n",
      "-------------------- Column: food --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97      6949\n",
      "          1       0.81      0.57      0.67       860\n",
      "\n",
      "avg / total       0.93      0.94      0.93      7809\n",
      "\n",
      "-------------------- Column: shelter --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97      7131\n",
      "          1       0.80      0.39      0.52       678\n",
      "\n",
      "avg / total       0.93      0.94      0.93      7809\n",
      "\n",
      "-------------------- Column: clothing --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      7698\n",
      "          1       0.64      0.08      0.14       111\n",
      "\n",
      "avg / total       0.98      0.99      0.98      7809\n",
      "\n",
      "-------------------- Column: money --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      7652\n",
      "          1       0.62      0.10      0.17       157\n",
      "\n",
      "avg / total       0.97      0.98      0.97      7809\n",
      "\n",
      "-------------------- Column: missing_people --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      7731\n",
      "          1       0.80      0.05      0.10        78\n",
      "\n",
      "avg / total       0.99      0.99      0.99      7809\n",
      "\n",
      "-------------------- Column: refugees --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      7552\n",
      "          1       0.59      0.07      0.12       257\n",
      "\n",
      "avg / total       0.96      0.97      0.96      7809\n",
      "\n",
      "-------------------- Column: death --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      7452\n",
      "          1       0.67      0.19      0.29       357\n",
      "\n",
      "avg / total       0.95      0.96      0.95      7809\n",
      "\n",
      "-------------------- Column: other_aid --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.99      0.93      6784\n",
      "          1       0.54      0.07      0.12      1025\n",
      "\n",
      "avg / total       0.83      0.87      0.82      7809\n",
      "\n",
      "-------------------- Column: infrastructure_related --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      7317\n",
      "          1       0.23      0.01      0.01       492\n",
      "\n",
      "avg / total       0.89      0.94      0.91      7809\n",
      "\n",
      "-------------------- Column: transport --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      7467\n",
      "          1       0.51      0.11      0.17       342\n",
      "\n",
      "avg / total       0.94      0.96      0.94      7809\n",
      "\n",
      "-------------------- Column: buildings --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98      7414\n",
      "          1       0.64      0.10      0.17       395\n",
      "\n",
      "avg / total       0.94      0.95      0.93      7809\n",
      "\n",
      "-------------------- Column: electricity --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      7662\n",
      "          1       0.80      0.08      0.15       147\n",
      "\n",
      "avg / total       0.98      0.98      0.98      7809\n",
      "\n",
      "-------------------- Column: tools --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      7764\n",
      "          1       0.00      0.00      0.00        45\n",
      "\n",
      "avg / total       0.99      0.99      0.99      7809\n",
      "\n",
      "-------------------- Column: hospitals --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      7717\n",
      "          1       0.00      0.00      0.00        92\n",
      "\n",
      "avg / total       0.98      0.99      0.98      7809\n",
      "\n",
      "-------------------- Column: shops --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      7784\n",
      "          1       0.00      0.00      0.00        25\n",
      "\n",
      "avg / total       0.99      1.00      1.00      7809\n",
      "\n",
      "-------------------- Column: aid_centers --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      7729\n",
      "          1       0.00      0.00      0.00        80\n",
      "\n",
      "avg / total       0.98      0.99      0.98      7809\n",
      "\n",
      "-------------------- Column: other_infrastructure --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      7468\n",
      "          1       0.00      0.00      0.00       341\n",
      "\n",
      "avg / total       0.91      0.95      0.93      7809\n",
      "\n",
      "-------------------- Column: weather_related --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      5653\n",
      "          1       0.84      0.64      0.73      2156\n",
      "\n",
      "avg / total       0.86      0.87      0.86      7809\n",
      "\n",
      "-------------------- Column: floods --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      7201\n",
      "          1       0.83      0.45      0.58       608\n",
      "\n",
      "avg / total       0.95      0.95      0.94      7809\n",
      "\n",
      "-------------------- Column: storm --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      7082\n",
      "          1       0.76      0.41      0.54       727\n",
      "\n",
      "avg / total       0.93      0.93      0.92      7809\n",
      "\n",
      "-------------------- Column: fire --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      7724\n",
      "          1       1.00      0.05      0.09        85\n",
      "\n",
      "avg / total       0.99      0.99      0.98      7809\n",
      "\n",
      "-------------------- Column: earthquake --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      7057\n",
      "          1       0.89      0.68      0.77       752\n",
      "\n",
      "avg / total       0.96      0.96      0.96      7809\n",
      "\n",
      "-------------------- Column: cold --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      7650\n",
      "          1       0.74      0.16      0.26       159\n",
      "\n",
      "avg / total       0.98      0.98      0.98      7809\n",
      "\n",
      "-------------------- Column: other_weather --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      7426\n",
      "          1       0.47      0.05      0.09       383\n",
      "\n",
      "avg / total       0.93      0.95      0.93      7809\n",
      "\n",
      "-------------------- Column: direct_report --------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91      6318\n",
      "          1       0.74      0.34      0.47      1491\n",
      "\n",
      "avg / total       0.84      0.85      0.83      7809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "TestModel(model, XTest['message'].values, yTest.values, yTest.columns, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "\n",
    "        Args:\n",
    "        y_true: array. Array containing actual labels.\n",
    "        y_pred: array. Array containing predicted labels.\n",
    "\n",
    "        Returns:\n",
    "        score: float. Median F1 score for all of the output classifiers\n",
    "        \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \n",
    "    'clf__estimator__min_samples_split' : [2, 5, 10],\n",
    "    'clf__estimator__n_estimators' : [10, 15, 20],\n",
    "    'tfidf__use_idf':[True], # definitely use it\n",
    "    'vect__min_df': [1, 5],\n",
    "    \n",
    "#    'clf__estimator__kernel': ['poly'], \n",
    "#    'clf__estimator__degree': [1, 2, 3],\n",
    "#    'clf__estimator__C':[1, 10, 100],\n",
    "    \n",
    "#    'clf__estimator__max_features' : [ 'auto', 'sqrt', 'log2'],\n",
    "#    'clf__estimator__max_depth' : [2, 4, 8],\n",
    "#    'clf__estimator__min_samples_split' : [10, 15, 20],\n",
    "#    'clf__estimator__min_samples_leaf' : [2, 5, 10],\n",
    "#    'clf__estimator__bootstrap' : [True, False],\n",
    "#    'clf__n_jobs' : [4],\n",
    "#    'clf__estimator__n_jobs' : [4],\n",
    "#    'clf__estimator__random_state' : [42],\n",
    "}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid = parameters,\n",
    "    \n",
    "    # doc: Controls the verbosity: \n",
    "    #      the higher, the more messages.\n",
    "    verbose = 10 ,\n",
    "    \n",
    "    scoring = scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.12356042678623323, total= 2.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.13484848484848486, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.1702638818925098, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  9.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.18825277207584662, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 12.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1431948665991219, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.17276384730538924, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 18.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1, score=0.16141001855287568, total= 2.2min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 22.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1, score=0.134006734006734, total= 2.2min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 25.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=1, score=0.16641367452501024, total= 2.2min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 29.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5, score=0.2079231692677071, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5, score=0.19900803410395146, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=15, tfidf__use_idf=True, vect__min_df=5, score=0.2188501095936604, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1, score=0.09290398288120384, total= 2.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1, score=0.09672568518069367, total= 2.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=1, score=0.15485969291279025, total= 2.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=5, score=0.1686618466279483, total= 2.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=5, score=0.1965742582294922, total= 2.1min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=20, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(105)\n",
    "impModel = cv.fit(XTrain['message'].values, yTrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impModel.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impModel.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestModel(impModel, XTest['message'].values, yTest.values, yPred, yTest.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When going through algorithms http://scikit-learn.org/stable/modules/multiclass.html I saw SVC and remembered that supported vector machines project data in higher spaces to separate data with an hyperplane. \n",
    "As bag of words is a large vector, I intend to find out if following algorithm works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cv = CountVectorizer(tokenizer = tokenize)\n",
    "_tfidf = TfidfTransformer()\n",
    "_classifierInner = SVC()\n",
    "_classifierOuter = MultiOutputClassifier(_classifierInner)\n",
    "\n",
    "\n",
    "pipeline2 = Pipeline([('vect', _cv),\n",
    "                     ('tfidf', _tfidf),\n",
    "                     ('clf', _classifierOuter,)\n",
    "                    ])\n",
    "\n",
    "parameters2 = {\n",
    "    \n",
    "#    'clf__estimator__min_samples_split' : [2, 5, 10],\n",
    "#    'clf__estimator__n_estimators' : [10, 15, 20],\n",
    "    'tfidf__use_idf':[True], # definitely use it\n",
    "    'vect__min_df': [1, 5],\n",
    "    \n",
    "    'clf__estimator__kernel': ['poly'], \n",
    "    'clf__estimator__degree': [1, 2, 3],\n",
    "    'clf__estimator__C':[1, 10, 50],\n",
    "    \n",
    "#    'clf__estimator__max_features' : [ 'auto', 'sqrt', 'log2'],\n",
    "#    'clf__estimator__max_depth' : [2, 4, 8],\n",
    "#    'clf__estimator__min_samples_split' : [10, 15, 20],\n",
    "#    'clf__estimator__min_samples_leaf' : [2, 5, 10],\n",
    "#    'clf__estimator__bootstrap' : [True, False],\n",
    "#    'clf__n_jobs' : [4],\n",
    "#    'clf__estimator__n_jobs' : [4],\n",
    "#    'clf__estimator__random_state' : [42],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = GridSearchCV(\n",
    "    pipeline2, \n",
    "    param_grid = parameters2,\n",
    "    \n",
    "    # doc: Controls the verbosity: \n",
    "    #      the higher, the more messages.\n",
    "    verbose = 10 ,\n",
    "    \n",
    "    scoring = scorer\n",
    ")\n",
    "\n",
    "np.random.seed(84)\n",
    "\n",
    "impModel2 = cv.fit(XTrain['message'].values, yTrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impModel2.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/34143829/sklearn-how-to-save-a-model-created-from-a-pipeline-and-gridsearchcv-using-jobli\n",
    "#If you want to dump your object into one file - use:  \n",
    "\n",
    "joblib.dump(bestEstimator, DisasterConfig.SaveFileJoblib, compress = 1)\n",
    "#pickle.dump(tuned_model, open(DisasterConfig.SaveFilePickle, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
